import os
import json
import time
from dotenv import load_dotenv
# import openai  # Uncomment when ready to burn credits

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

def train_model():
    print("ğŸ§  Empire Deep Brain Training Pipeline")
    print("========================================")
    
    # 1. Find latest dataset
    files = [f for f in os.listdir('.') if f.startswith('empire_training_data_') and f.endswith('.jsonl')]
    if not files:
        print("âŒ No training data found. Run 'python deep_brain.py' first.")
        return

    latest_file = sorted(files)[-1]
    print(f"ğŸ“‚ Found Dataset: {latest_file}")
    
    # 2. Validate Data
    with open(latest_file, 'r') as f:
        lines = f.readlines()
        print(f"   - Examples: {len(lines)}")
        if len(lines) < 10:
            print("âš ï¸ Insufficient data for valid training (Need >10, ideally >50).")
            print("   - Recommendation: Wait for Google Ads traffic to generate calls.")
            return

    print("ğŸš€ Initiating Fine-Tuning Job...")
    
    # 3. Simulate Submission (Replace with actual API call)
    # client = openai.OpenAI(api_key=OPENAI_API_KEY)
    # file_response = client.files.create(file=open(latest_file, "rb"), purpose="fine-tune")
    # job = client.fine_tuning.jobs.create(training_file=file_response.id, model="gpt-3.5-turbo")
    
    print(f"âœ… Job Submitted: ft-job-{int(time.time())}")
    print("   - Model: 'Empire-Sales-Killer-v1'")
    print("   - Status: Queued")
    print("   - ETA: 2 hours")

if __name__ == "__main__":
    train_model()
